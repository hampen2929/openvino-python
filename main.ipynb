{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# '/opt/intel/openvino_2019.2.242/python/python3.7' doesn't work \n",
    "sys.path[1] = '/opt/intel/openvino_2019.2.242/python/python3.6'\n",
    "\n",
    "from openvino.inference_engine import IENetwork, IEPlugin, IECore\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.1-openvino'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"CPU\"\n",
    "plugin = IEPlugin(device=device, plugin_dirs=None)\n",
    "if device == \"CPU\":\n",
    "    plugin.add_cpu_extension(\"./deployment_tools/inference_engine/lib/intel64/libcpu_extension.dylib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if \"CPU\" in device:\n",
    "    supported_layers = ie.query_network(net, \"CPU\")\n",
    "    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "    if len(not_supported_layers) != 0:\n",
    "        log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                  format(args.device, ', '.join(not_supported_layers)))\n",
    "        log.error(\"Please try to specify cpu extensions library path in sample's command line parameters using -l \"\n",
    "                  \"or --cpu_extension command line argument\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = ['detect_face', 'emotion_recognition', 'estimate_headpose', 'detect_person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_dir = './model/'\n",
    "tasks = {'detect_face':'face-detection-adas-0001.xml',\n",
    "                'emotion_recognition': 'emotions-recognition-retail-0003.xml',\n",
    "                'estimate_headpose': 'head-pose-estimation-adas-0001.xml', \n",
    "                'detect_person': 'person-detection-retail-0002.xml'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_extension = \"./deployment_tools/inference_engine/lib/intel64/libcpu_extension.dylib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self, device='CPU', input_='cam', ):\n",
    "        self.device = device\n",
    "        self.input = input_\n",
    "        self.labels=None\n",
    "        self.prob_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    # TODO: load from config\n",
    "    def __init__(self, task, device=\"CPU\"):\n",
    "        self.task = task\n",
    "        self._set_model_path()\n",
    "        # Read IR\n",
    "        self.net = IENetwork(model=self.model_xml, weights=self.model_bin)\n",
    "        # Load Model\n",
    "        self._set_ieplugin()\n",
    "        \n",
    "    def _set_ieplugin(self):\n",
    "        plugin = IEPlugin(device=device, plugin_dirs=None)\n",
    "        if device == \"CPU\":\n",
    "            plugin.add_cpu_extension(path_extension)\n",
    "        \n",
    "        self.exec_net = plugin.load(network=self.net, num_requests=2)\n",
    "        self._get_io_blob()\n",
    "\n",
    "    def _set_model_path(self):\n",
    "        model_name = tasks[self.task]\n",
    "\n",
    "        self.model_xml = os.path.join(path_model_dir, model_name)\n",
    "        self.model_bin = os.path.splitext(self.model_xml)[0] + \".bin\"\n",
    "    \n",
    "    def _get_io_blob(self):\n",
    "        self.input_blob = next(iter(self.net.inputs))\n",
    "        self.out_blob = next(iter(self.net.outputs))\n",
    "    \n",
    "    def _in_frame(self, frame, n, c, h, w):\n",
    "        in_frame = cv2.resize(frame, (w, h))\n",
    "        in_frame = in_frame.transpose((2, 0, 1))\n",
    "        in_frame = in_frame.reshape((n, c, h, w))\n",
    "        return in_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDetectPerson(Model):\n",
    "    def __init__(self, task):\n",
    "        super().__init__(task)\n",
    "        self._set_iecore()\n",
    "    \n",
    "    def _set_iecore(self):\n",
    "        self.ie = IECore()\n",
    "        self.ie.add_extension(path_extension, \"CPU\")\n",
    "        self.exec_net = self.ie.load_network(network=self.net, num_requests=2, device_name=args.device)\n",
    "        \n",
    "        if \"CPU\" in device:\n",
    "            supported_layers = self.ie.query_network(self.net, \"CPU\")\n",
    "            not_supported_layers = [l for l in self.net.layers.keys() if l not in supported_layers]\n",
    "            if len(not_supported_layers) != 0:\n",
    "                log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                          format(args.device, ', '.join(not_supported_layers)))\n",
    "                log.error(\"Please try to specify cpu extensions library path in sample's command line parameters using -l \"\n",
    "                          \"or --cpu_extension command line argument\")\n",
    "                sys.exit(1)\n",
    "        \n",
    "    def compute(self):\n",
    "        img_info_input_blob = None\n",
    "        feed_dict = {}\n",
    "        for blob_name in self.net.inputs:\n",
    "            if len(self.net.inputs[blob_name].shape) == 4:\n",
    "                input_blob = blob_name\n",
    "            elif len(self.net.inputs[blob_name].shape) == 2:\n",
    "                img_info_input_blob = blob_name\n",
    "            else:\n",
    "                raise RuntimeError(\"Unsupported {}D input layer '{}'. Only 2D and 4D input layers are supported\"\n",
    "                                   .format(len(net.inputs[blob_name].shape), blob_name))\n",
    "\n",
    "        assert len(self.net.outputs) == 1, \"Demo supports only single output topologies\"\n",
    "        \n",
    "        out_blob = next(iter(self.net.outputs))\n",
    "        log.info(\"Loading IR to the plugin...\")\n",
    "        # exec_net = self.ie.load_network(network=self.net, num_requests=2, device_name=args.device)\n",
    "        # Read and pre-process input image\n",
    "        n, c, h, w = self.net.inputs[input_blob].shape\n",
    "        if img_info_input_blob:\n",
    "            feed_dict[img_info_input_blob] = [h, w, 1]\n",
    "\n",
    "        if args.input == 'cam':\n",
    "            input_stream = 0\n",
    "        else:\n",
    "            input_stream = args.input\n",
    "            assert os.path.isfile(args.input), \"Specified input file doesn't exist\"\n",
    "        if args.labels:\n",
    "            with open(args.labels, 'r') as f:\n",
    "                labels_map = [x.strip() for x in f]\n",
    "        else:\n",
    "            labels_map = None\n",
    "\n",
    "        cap = cv2.VideoCapture(input_stream)\n",
    "\n",
    "        cur_request_id = 0\n",
    "        next_request_id = 1\n",
    "\n",
    "        log.info(\"Starting inference in async mode...\")\n",
    "        is_async_mode = True\n",
    "        render_time = 0\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        print(\"To close the application, press 'CTRL+C' here or switch to the output window and press ESC key\")\n",
    "        print(\"To switch between sync/async modes, press TAB key in the output window\")\n",
    "\n",
    "        while cap.isOpened():\n",
    "            if is_async_mode:\n",
    "                ret, next_frame = cap.read()\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            initial_w = cap.get(3)\n",
    "            initial_h = cap.get(4)\n",
    "            # Main sync point:\n",
    "            # in the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete\n",
    "            # in the regular mode we start the CURRENT request and immediately wait for it's completion\n",
    "            inf_start = time.time()\n",
    "            if is_async_mode:\n",
    "                in_frame = cv2.resize(next_frame, (w, h))\n",
    "                in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "                in_frame = in_frame.reshape((n, c, h, w))\n",
    "                feed_dict[self.input_blob] = in_frame\n",
    "                # import pdb; pdb.set_trace()\n",
    "                self.exec_net.start_async(request_id=next_request_id, inputs=feed_dict)\n",
    "    \n",
    "            else:\n",
    "                in_frame = cv2.resize(frame, (w, h))\n",
    "                in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "                in_frame = in_frame.reshape((n, c, h, w))\n",
    "                feed_dict[input_blob] = in_frame\n",
    "                self.exec_net.start_async(request_id=cur_request_id, inputs=feed_dict)\n",
    "            if self.exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "                inf_end = time.time()\n",
    "                det_time = inf_end - inf_start\n",
    "\n",
    "                # Parse detection results of the current request\n",
    "                res = self.exec_net.requests[cur_request_id].outputs[out_blob]\n",
    "                for obj in res[0][0]:\n",
    "                    # Draw only objects when probability more than specified threshold\n",
    "                    if obj[2] > args.prob_threshold:\n",
    "                        xmin = int(obj[3] * initial_w)\n",
    "                        ymin = int(obj[4] * initial_h)\n",
    "                        xmax = int(obj[5] * initial_w)\n",
    "                        ymax = int(obj[6] * initial_h)\n",
    "                        class_id = int(obj[1])\n",
    "                        # Draw box and label\\class_id\n",
    "                        color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "                        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                        det_label = labels_map[class_id] if labels_map else str(class_id)\n",
    "                        cv2.putText(frame, det_label + ' ' + str(round(obj[2] * 100, 1)) + ' %', (xmin, ymin - 7),\n",
    "                                    cv2.FONT_HERSHEY_COMPLEX, 0.6, color, 1)\n",
    "\n",
    "                # Draw performance stats\n",
    "                inf_time_message = \"Inference time: N\\A for async mode\" if is_async_mode else \\\n",
    "                    \"Inference time: {:.3f} ms\".format(det_time * 1000)\n",
    "                render_time_message = \"OpenCV rendering time: {:.3f} ms\".format(render_time * 1000)\n",
    "                async_mode_message = \"Async mode is on. Processing request {}\".format(cur_request_id) if is_async_mode else \\\n",
    "                    \"Async mode is off. Processing request {}\".format(cur_request_id)\n",
    "\n",
    "                cv2.putText(frame, inf_time_message, (15, 15), cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "                cv2.putText(frame, render_time_message, (15, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (10, 10, 200), 1)\n",
    "                cv2.putText(frame, async_mode_message, (10, int(initial_h - 20)), cv2.FONT_HERSHEY_COMPLEX, 0.5,\n",
    "                            (10, 10, 200), 1)\n",
    "\n",
    "            #\n",
    "            render_start = time.time()\n",
    "            cv2.imshow(\"Detection Results\", frame)\n",
    "            render_end = time.time()\n",
    "            render_time = render_end - render_start\n",
    "\n",
    "            if is_async_mode:\n",
    "                cur_request_id, next_request_id = next_request_id, cur_request_id\n",
    "                frame = next_frame\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == 27:\n",
    "                break\n",
    "            if (9 == key):\n",
    "                is_async_mode = not is_async_mode\n",
    "                log.info(\"Switched to {} mode\".format(\"async\" if is_async_mode else \"sync\"))\n",
    "\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dp = ModelDetectPerson('detect_person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To close the application, press 'CTRL+C' here or switch to the output window and press ESC key\n",
      "To switch between sync/async modes, press TAB key in the output window\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3) into shape (1,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-a6185e7e6156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-fd8150ea2986>\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_blob\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m# import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_request_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.ExecutableNetwork.start_async\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.InferRequest.async_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.InferRequest.async_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.InferRequest._fill_inputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (3) into shape (1,6)"
     ]
    }
   ],
   "source": [
    "model_dp.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDetectPerson(Model):\n",
    "    def __init__(self, task):\n",
    "        super().__init__(task)\n",
    "        self._set_iecore()\n",
    "    \n",
    "    def _set_iecore(self):\n",
    "        self.ie = IECore()\n",
    "        self.ie.add_extension(path_extension, \"CPU\")\n",
    "        self.exec_net = self.ie.load_network(network=self.net, num_requests=2, device_name=args.device)\n",
    "        \n",
    "        if \"CPU\" in device:\n",
    "            supported_layers = self.ie.query_network(self.net, \"CPU\")\n",
    "            not_supported_layers = [l for l in self.net.layers.keys() if l not in supported_layers]\n",
    "            if len(not_supported_layers) != 0:\n",
    "                log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                          format(args.device, ', '.join(not_supported_layers)))\n",
    "                log.error(\"Please try to specify cpu extensions library path in sample's command line parameters using -l \"\n",
    "                          \"or --cpu_extension command line argument\")\n",
    "                sys.exit(1)\n",
    "        \n",
    "    def main(self):\n",
    "        img_info_input_blob = None\n",
    "        feed_dict = {}\n",
    "        for blob_name in self.net.inputs:\n",
    "            if len(self.net.inputs[blob_name].shape) == 4:\n",
    "                input_blob = blob_name\n",
    "            elif len(self.net.inputs[blob_name].shape) == 2:\n",
    "                img_info_input_blob = blob_name\n",
    "            else:\n",
    "                raise RuntimeError(\"Unsupported {}D input layer '{}'. Only 2D and 4D input layers are supported\"\n",
    "                                   .format(len(net.inputs[blob_name].shape), blob_name))\n",
    "\n",
    "        assert len(self.net.outputs) == 1, \"Demo supports only single output topologies\"\n",
    "        \n",
    "        out_blob = next(iter(self.net.outputs))\n",
    "        log.info(\"Loading IR to the plugin...\")\n",
    "        # exec_net = self.ie.load_network(network=self.net, num_requests=2, device_name=args.device)\n",
    "        # Read and pre-process input image\n",
    "        n, c, h, w = self.net.inputs[input_blob].shape\n",
    "        if img_info_input_blob:\n",
    "            feed_dict[img_info_input_blob] = [h, w, 1]\n",
    "\n",
    "        if args.input == 'cam':\n",
    "            input_stream = 0\n",
    "        else:\n",
    "            input_stream = args.input\n",
    "            assert os.path.isfile(args.input), \"Specified input file doesn't exist\"\n",
    "        if args.labels:\n",
    "            with open(args.labels, 'r') as f:\n",
    "                labels_map = [x.strip() for x in f]\n",
    "        else:\n",
    "            labels_map = None\n",
    "\n",
    "        cap = cv2.VideoCapture(input_stream)\n",
    "\n",
    "        cur_request_id = 0\n",
    "        next_request_id = 1\n",
    "\n",
    "        log.info(\"Starting inference in async mode...\")\n",
    "        is_async_mode = True\n",
    "        render_time = 0\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        print(\"To close the application, press 'CTRL+C' here or switch to the output window and press ESC key\")\n",
    "        print(\"To switch between sync/async modes, press TAB key in the output window\")\n",
    "\n",
    "        while cap.isOpened():\n",
    "            if is_async_mode:\n",
    "                ret, next_frame = cap.read()\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            initial_w = cap.get(3)\n",
    "            initial_h = cap.get(4)\n",
    "            # Main sync point:\n",
    "            # in the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete\n",
    "            # in the regular mode we start the CURRENT request and immediately wait for it's completion\n",
    "            inf_start = time.time()\n",
    "            if is_async_mode:\n",
    "                in_frame = cv2.resize(next_frame, (w, h))\n",
    "                in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "                in_frame = in_frame.reshape((n, c, h, w))\n",
    "                feed_dict[self.input_blob] = in_frame\n",
    "                # import pdb; pdb.set_trace()\n",
    "                self.exec_net.start_async(request_id=next_request_id, inputs=feed_dict)\n",
    "    \n",
    "            else:\n",
    "                in_frame = cv2.resize(frame, (w, h))\n",
    "                in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "                in_frame = in_frame.reshape((n, c, h, w))\n",
    "                feed_dict[input_blob] = in_frame\n",
    "                self.exec_net.start_async(request_id=cur_request_id, inputs=feed_dict)\n",
    "            if self.exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "                inf_end = time.time()\n",
    "                det_time = inf_end - inf_start\n",
    "\n",
    "                # Parse detection results of the current request\n",
    "                res = self.exec_net.requests[cur_request_id].outputs[out_blob]\n",
    "                for obj in res[0][0]:\n",
    "                    # Draw only objects when probability more than specified threshold\n",
    "                    if obj[2] > args.prob_threshold:\n",
    "                        xmin = int(obj[3] * initial_w)\n",
    "                        ymin = int(obj[4] * initial_h)\n",
    "                        xmax = int(obj[5] * initial_w)\n",
    "                        ymax = int(obj[6] * initial_h)\n",
    "                        class_id = int(obj[1])\n",
    "                        # Draw box and label\\class_id\n",
    "                        color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "                        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                        det_label = labels_map[class_id] if labels_map else str(class_id)\n",
    "                        cv2.putText(frame, det_label + ' ' + str(round(obj[2] * 100, 1)) + ' %', (xmin, ymin - 7),\n",
    "                                    cv2.FONT_HERSHEY_COMPLEX, 0.6, color, 1)\n",
    "\n",
    "                # Draw performance stats\n",
    "                inf_time_message = \"Inference time: N\\A for async mode\" if is_async_mode else \\\n",
    "                    \"Inference time: {:.3f} ms\".format(det_time * 1000)\n",
    "                render_time_message = \"OpenCV rendering time: {:.3f} ms\".format(render_time * 1000)\n",
    "                async_mode_message = \"Async mode is on. Processing request {}\".format(cur_request_id) if is_async_mode else \\\n",
    "                    \"Async mode is off. Processing request {}\".format(cur_request_id)\n",
    "\n",
    "                cv2.putText(frame, inf_time_message, (15, 15), cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "                cv2.putText(frame, render_time_message, (15, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (10, 10, 200), 1)\n",
    "                cv2.putText(frame, async_mode_message, (10, int(initial_h - 20)), cv2.FONT_HERSHEY_COMPLEX, 0.5,\n",
    "                            (10, 10, 200), 1)\n",
    "\n",
    "            #\n",
    "            render_start = time.time()\n",
    "            cv2.imshow(\"Detection Results\", frame)\n",
    "            render_end = time.time()\n",
    "            render_time = render_end - render_start\n",
    "\n",
    "            if is_async_mode:\n",
    "                cur_request_id, next_request_id = next_request_id, cur_request_id\n",
    "                frame = next_frame\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == 27:\n",
    "                break\n",
    "            if (9 == key):\n",
    "                is_async_mode = not is_async_mode\n",
    "                log.info(\"Switched to {} mode\".format(\"async\" if is_async_mode else \"sync\"))\n",
    "\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/yuya/src/pedestrian_traker/ie_api.pyx\u001b[0m(368)\u001b[0;36mopenvino.inference_engine.ie_api.InferRequest._fill_inputs\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/yuya/src/pedestrian_traker/ie_api.pyx\u001b[0m(368)\u001b[0;36mopenvino.inference_engine.ie_api.InferRequest._fill_inputs\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDetectFace(Model):\n",
    "    # TODO: load from config    \n",
    "    def get_face_pos(self, frame):\n",
    "        n, c, h, w = self.net.inputs[self.input_blob].shape\n",
    "        self.shapes = (n, c, h, w)\n",
    "        scale = 640 / frame.shape[1]\n",
    "        frame = cv2.resize(frame, dsize=None, fx=scale, fy=scale)\n",
    "        \n",
    "        self.frame_h, self.frame_w = frame.shape[:2]\n",
    "        init_frame = frame.copy()\n",
    "\n",
    "        in_frame = self._in_frame(frame, n, c, h, w)\n",
    "        self.exec_net.start_async(request_id=0, inputs={self.input_blob: in_frame}) # res's shape: [1, 1, 200, 7]\n",
    "\n",
    "        if self.exec_net.requests[0].wait(-1) == 0:\n",
    "            res = self.exec_net.requests[0].outputs[self.out_blob]\n",
    "            faces = res[0][:, np.where(res[0][0][:, 2] > 0.5)] # prob threshold : 0.5\n",
    "        return faces\n",
    "    \n",
    "    def detect_face(self, frame):\n",
    "        faces = self.get_face_pos(frame)\n",
    "        \n",
    "        # frame = init_frame.copy()\n",
    "        for face in faces[0][0]:\n",
    "            box = face[3:7] * np.array([self.frame_w, self.frame_h, self.frame_w, self.frame_h])\n",
    "            (xmin, ymin, xmax, ymax) = box.astype(\"int\")\n",
    "            \"\"\"\n",
    "            xmin = int(face[3] * frame_w)\n",
    "            ymin = int(face[4] * frame_h)\n",
    "            xmax = int(face[5] * frame_w)\n",
    "            ymax = int(face[6] * frame_h)\n",
    "            \"\"\"\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ('neutral', 'happy', 'sad', 'surprise', 'anger')\n",
    "# plot setting\n",
    "rows = 6\n",
    "columns = 6\n",
    "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "figsize = (8, 8)\n",
    "\n",
    "class ModelEmotionRecognition(Model):\n",
    " \n",
    "    def emotion_recognition(self, frame, faces, rect):# 4. Create Async Request\n",
    "        frame_h, frame_w = frame.shape[:2]\n",
    "        n, c, h, w = self.net.inputs[self.input_blob].shape\n",
    "        face_id = 0\n",
    "        for face in faces[0][0]:\n",
    "            box = face[3:7] * np.array([frame_w, frame_h, frame_w, frame_h])\n",
    "            (xmin, ymin, xmax, ymax) = box.astype(\"int\")\n",
    "            face_frame = frame[ymin:ymax, xmin:xmax]\n",
    "            \n",
    "            if (face_frame.shape[0]==0) or (face_frame.shape[1]==0):\n",
    "                continue\n",
    "            \n",
    "            in_frame = self._in_frame(frame, n, c, h, w)\n",
    "            self.exec_net.start_async(request_id=0 ,inputs={self.input_blob: in_frame})\n",
    "\n",
    "            # 5. Get reponse\n",
    "            if self.exec_net.requests[0].wait(-1) == 0:\n",
    "                res = self.exec_net.requests[0].outputs[self.out_blob]\n",
    "                emotion = label[np.argmax(res[0])]\n",
    "                ax = plt.subplot(rows, columns, face_id + 1)\n",
    "                ax.set_title(\"{}\".format(emotion))\n",
    "                plt.imshow(face_frame)\n",
    "                face_id +=1\n",
    "            \n",
    "            if rect:\n",
    "                frame = cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, emotion, (20,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,0,200), 2, cv2.LINE_AA)\n",
    "        \n",
    "        return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEstimateHeadpose(Model):\n",
    "    def _build_camera_matrix(self, center_of_face, focal_length):\n",
    "    \n",
    "        cx = int(center_of_face[0])\n",
    "        cy = int(center_of_face[1])\n",
    "        camera_matrix = np.zeros((3, 3), dtype='float32')\n",
    "        camera_matrix[0][0] = focal_length\n",
    "        camera_matrix[0][2] = cx\n",
    "        camera_matrix[1][1] = focal_length\n",
    "        camera_matrix[1][2] = cy\n",
    "        camera_matrix[2][2] = 1\n",
    "\n",
    "        return camera_matrix\n",
    "\n",
    "    def _draw_axes(self, frame, center_of_face, yaw, pitch, roll, scale, focal_length):\n",
    "        yaw *= np.pi / 180.0\n",
    "        pitch *= np.pi / 180.0\n",
    "        roll *= np.pi / 180.0\n",
    "\n",
    "        cx = int(center_of_face[0])\n",
    "        cy = int(center_of_face[1])\n",
    "        Rx = np.array([[1,                0,                               0],\n",
    "                       [0,                math.cos(pitch),  -math.sin(pitch)],\n",
    "                       [0,                math.sin(pitch),   math.cos(pitch)]])\n",
    "        Ry = np.array([[math.cos(yaw),    0,                  -math.sin(yaw)],\n",
    "                       [0,                1,                               0],\n",
    "                       [math.sin(yaw),    0,                   math.cos(yaw)]])\n",
    "        Rz = np.array([[math.cos(roll),   -math.sin(roll),                 0],\n",
    "                       [math.sin(roll),   math.cos(roll),                  0],\n",
    "                       [0,                0,                               1]]) \n",
    "\n",
    "        #R = np.dot(Rz, Ry, Rx)\n",
    "        #ref: https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "        #R = np.dot(Rz, np.dot(Ry, Rx))\n",
    "        R = Rz @ Ry @ Rx\n",
    "        # print(R)\n",
    "        camera_matrix = self._build_camera_matrix(center_of_face, focal_length)\n",
    "\n",
    "        xaxis = np.array(([1 * scale, 0, 0]), dtype='float32').reshape(3, 1)\n",
    "        yaxis = np.array(([0, -1 * scale, 0]), dtype='float32').reshape(3, 1)\n",
    "        zaxis = np.array(([0, 0, -1 * scale]), dtype='float32').reshape(3, 1)\n",
    "        zaxis1 = np.array(([0, 0, 1 * scale]), dtype='float32').reshape(3, 1)\n",
    "\n",
    "        o = np.array(([0, 0, 0]), dtype='float32').reshape(3, 1)\n",
    "        o[2] = camera_matrix[0][0]\n",
    "\n",
    "        xaxis = np.dot(R, xaxis) + o\n",
    "        yaxis = np.dot(R, yaxis) + o\n",
    "        zaxis = np.dot(R, zaxis) + o\n",
    "        zaxis1 = np.dot(R, zaxis1) + o\n",
    "\n",
    "        xp2 = (xaxis[0] / xaxis[2] * camera_matrix[0][0]) + cx\n",
    "        yp2 = (xaxis[1] / xaxis[2] * camera_matrix[1][1]) + cy\n",
    "        p2 = (int(xp2), int(yp2))\n",
    "        cv2.line(frame, (cx, cy), p2, (0, 0, 255), 2)\n",
    "\n",
    "        xp2 = (yaxis[0] / yaxis[2] * camera_matrix[0][0]) + cx\n",
    "        yp2 = (yaxis[1] / yaxis[2] * camera_matrix[1][1]) + cy\n",
    "        p2 = (int(xp2), int(yp2))\n",
    "        cv2.line(frame, (cx, cy), p2, (0, 255, 0), 2)\n",
    "\n",
    "        xp1 = (zaxis1[0] / zaxis1[2] * camera_matrix[0][0]) + cx\n",
    "        yp1 = (zaxis1[1] / zaxis1[2] * camera_matrix[1][1]) + cy\n",
    "        p1 = (int(xp1), int(yp1))\n",
    "        xp2 = (zaxis[0] / zaxis[2] * camera_matrix[0][0]) + cx\n",
    "        yp2 = (zaxis[1] / zaxis[2] * camera_matrix[1][1]) + cy\n",
    "        p2 = (int(xp2), int(yp2))\n",
    "\n",
    "        cv2.line(frame, p1, p2, (255, 0, 0), 2)\n",
    "        cv2.circle(frame, p2, 3, (255, 0, 0), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def estimate_headpose(self, frame, faces):\n",
    "        # 4. Create Async Request\n",
    "        scale = 50\n",
    "        focal_length = 950.0\n",
    "        frame_h, frame_w = frame.shape[:2]\n",
    "        \n",
    "        n, c, h, w = self.net.inputs[self.input_blob].shape\n",
    "        \n",
    "        if len(faces)>0:\n",
    "            for face in faces[0][0]:\n",
    "                box = face[3:7] * np.array([frame_w, frame_h, frame_w, frame_h])\n",
    "                (xmin, ymin, xmax, ymax) = box.astype(\"int\")\n",
    "                face_frame = frame[ymin:ymax, xmin:xmax]\n",
    "                \n",
    "                if (face_frame.shape[0]==0) or (face_frame.shape[1]==0):\n",
    "                    continue\n",
    "                in_frame = self._in_frame(frame, n, c, h, w)                \n",
    "                self.exec_net.start_async(request_id=0 ,inputs={self.input_blob: in_frame})\n",
    "                if self.exec_net.requests[0].wait(-1) == 0:\n",
    "                    yaw = .0  # Axis of rotation: y\n",
    "                    pitch = .0  # Axis of rotation: x\n",
    "                    roll = .0  # Axis of rotation: z\n",
    "                    # Each output contains one float value that represents value in Tait-Bryan angles (yaw, pitсh or roll).\n",
    "                    yaw = self.exec_net.requests[0].outputs['angle_y_fc'][0][0]\n",
    "                    pitch = self.exec_net.requests[0].outputs['angle_p_fc'][0][0]\n",
    "                    roll = self.exec_net.requests[0].outputs['angle_r_fc'][0][0]\n",
    "                    # print(\"yaw:{:f}, pitch:{:f}, roll:{:f}\".format(yaw, pitch, roll))\n",
    "                    center_of_face = (xmin + face_frame.shape[1] / 2, ymin + face_frame.shape[0] / 2, 0)\n",
    "                    self._draw_axes(frame, center_of_face, yaw, pitch, roll, scale, focal_length)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return frame    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(object):\n",
    "    def __init__(self, task='detect_face', rect=True):\n",
    "        self.task = task\n",
    "        self.rect = rect\n",
    "        self._set_model()\n",
    "        \n",
    "    def _set_model(self):\n",
    "        if self.task == 'detect_face':\n",
    "            self.model_df = ModelDetectFace('detect_face')\n",
    "        elif self.task == 'emotion_recognition':\n",
    "            self.model_df = ModelDetectFace('detect_face')\n",
    "            self.model_er = ModelEmotionRecognition('emotion_recognition')        \n",
    "        elif self.task == 'estimate_headpose':\n",
    "            self.model_df = ModelDetectFace('detect_face')\n",
    "            self.model_eh = ModelEstimateHeadpose('estimate_headpose')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def compute(self, frame):\n",
    "        if self.task == 'detect_face':\n",
    "            frame = self.model_df.detect_face(frame)\n",
    "        elif self.task == 'emotion_recognition':\n",
    "            faces = self.model_df.get_face_pos(frame)\n",
    "            frame = self.model_er.emotion_recognition(frame, faces, self.rect)\n",
    "        elif self.task == 'estimate_headpose':\n",
    "            faces = self.model_df.get_face_pos(frame)\n",
    "            frame = self.model_eh.estimate_headpose(frame, faces)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task = 'detect_face'\n",
    "#task = 'emotion_recognition'\n",
    "task = 'estimate_headpose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = np.array(frame)\n",
    "\n",
    "    #######\n",
    "    \n",
    "    frame = detector.compute(frame)\n",
    "    \n",
    "    #######\n",
    "    cv2.imshow('demo', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
